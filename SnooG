# -*- coding: utf-8 -*-
"""SnooG_TEST0053.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d4nkQ1v-govsVRP5bos5VS-sZ5r2No3p

#**SnooG Alogarithm**

## **Packages**

Installing
"""

pip install Flask

pip install yahoofinancials

pip install yfinance

pip install pmdarima

pip install chart_studio



"""##Modules"""

import os
from pathlib import Path
import numpy as np
import pandas as pd
from datetime import date
import scipy
from numpy.random import normal, seed
from scipy.stats import norm
from dateutil.parser import parse
import itertools
import requests
from bs4 import BeautifulSoup
import base64
from flask import Flask, render_template, request, jsonify
from tabulate import tabulate
import tempfile
from googlesearch import search

# Visualization Time Series Data
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from pylab import rcParams
from plotly import tools
import plotly.offline as offline
import plotly.figure_factory as ff
import plotly as py
from plotly.offline import init_notebook_mode, iplot
from plotly.subplots import make_subplots
import cufflinks as cf

# Import yfinance package
import yfinance as yf
from yahoofinancials import YahooFinancials

#Statistics
import math
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.metrics import (
    roc_curve,
    auc,
    confusion_matrix,
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    matthews_corrcoef,
)

import statsmodels.api as sm
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.stattools import adfuller, kpss
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.arima_process import ArmaProcess
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import acf, pacf
from statsmodels.graphics.tsaplots import plot_acf,plot_pacf
from statsmodels.graphics.tsaplots import plot_predict

from pandas.plotting import autocorrelation_plot
from pmdarima.arima import auto_arima
import pmdarima as pmd

from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.cluster import KMeans
from scipy.signal import find_peaks

from multiprocessing import cpu_count
from joblib import Parallel
from joblib import delayed
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout, Masking, Embedding, GRU, Bidirectional

from IPython.display import HTML

"""##Functions

##Main

*Getting Started*
"""

class Onono:
    def __init__(self):
        pass

    def search_results(self):
        query = input("Enter your search query: ")
        links = []
        try:
            for i, result in enumerate(search(query, num=10, stop=10, pause=2), start=1):
                link = f"{i}. {result}"
                links.append(link)
            return links
        except Exception as e:
            print(f"An error occurred: {e}")
            return []

    def create_download_link(self, df, title="Download CSV file", filename="Snoog_Analytics.csv"):
        if not isinstance(df, pd.DataFrame):
            raise ValueError("df must be a Pandas DataFrame object.")

        with tempfile.NamedTemporaryFile(delete=False, suffix='.csv') as temp_file:
            df.to_csv(temp_file.name, index=False)
            temp_file.seek(0)  # Move to the beginning of the file

            b64 = base64.b64encode(temp_file.read()).decode()

            html = f'<a download="{filename}" href="data:text/csv;base64,{b64}" target="_blank">{title}</a>'
            return HTML(html)

    def create_and_set_working_directory(self, root_dir, project_folder):
        try:
            project_path = Path(root_dir) / project_folder
            project_path.mkdir(parents=True, exist_ok=True)
            print(f"{project_path} did not exist but was created.")

            os.chdir(str(project_path))

            test_file_path = project_path / "new_file_in_working_directory.txt"
            test_file_path.touch()

            print(f"\nYour working directory was changed to {project_path}\n"
                  f"\nAn empty text file was created there. "
                  f"You can also run !pwd to confirm the current working directory.")
        except Exception as e:
            print(f"An error occurred: {e}")


#Getting the data
class CurrencyDataProcessor:
    def __init__(self, tickers):
        self.tickers = tickers
        self.currency_data = {}

    def fetch_currency_data(self, start_date, end_date, interval='1d'):
        valid_intervals = ['1d', '1wk', '1mo']
        if interval not in valid_intervals:
            raise ValueError(f"Invalid interval. Please choose from {valid_intervals}.")

        for ticker in self.tickers:
            ticker = ticker.upper()  # Convert the ticker to uppercase to ensure consistency

            # Append '=X' to the ticker if not already present
            if '=X' not in ticker:
                ticker = ticker + '=X'

            try:
                data = yf.download(ticker, start=start_date, end=end_date, interval=interval)
                if data.empty:
                    raise ValueError(f"No data available for the ticker: {ticker}")
                currency_pair = ticker.replace('=X', '')

                # Check if the data has the expected columns
                expected_columns = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']
                if not all(col in data.columns for col in expected_columns):
                    raise ValueError(f"Data for the ticker '{ticker}' does not have the expected columns.")
                # Check if the data has the expected data types
                expected_dtypes = {'Open': float, 'High': float, 'Low': float, 'Close': float, 'Adj Close': float, 'Volume': int}
                if not all(data[col].dtype == expected_dtypes[col] for col in expected_dtypes):
                    raise ValueError(f"Data for the ticker '{ticker}' does not have the expected data types.")
                self.currency_data[currency_pair] = data
            except Exception as e:
                raise ValueError(f"Error while fetching data for the ticker '{ticker}': {str(e)}")

        return self.currency_data

    def Convert2dataframe(self, df):
        if isinstance(df, tuple):
            df = pd.DataFrame(df)

        if isinstance(df, pd.DataFrame):
            print("Data is a DataFrame.")
        else:
            raise ValueError("Error: Data is not a DataFrame.")

        # Update the input DataFrame in place
        df.update(df)

        return df

    def main(self):
        # Print available currency pairs
        print("Available currency pairs:")
        print(", ".join(self.currency_data.keys()))

        # Get user input for the desired currency pair
        currency_choice = input("Enter Currency Pair (e.g., EURGBP): ").strip()

        # Ensure that currency_data is a dictionary before accessing its keys
        if isinstance(self.currency_data, dict):
            # Check if the chosen currency pair exists in the dictionary
            if currency_choice in self.currency_data:
                # Access the DataFrame for the chosen currency pair
                Data = self.currency_data[currency_choice]

                # Print the head of the DataFrame
                print("\nHead of the DataFrame for the chosen currency pair:")
                print(Data.head())

            else:
                print(f"Error: '{currency_choice}' is not a valid currency pair.")
        else:
            print("Error: 'currency_data' is not a dictionary.")

class DataPreprocessor:
    def __init__(self, impute_method='knn', normalization=True):
        self.impute_method = impute_method
        self.normalization = normalization

    def missing_zero_values_table(self, df):
        zero_val = (df == 0.00).astype(int).sum(axis=0)
        mis_val = df.isnull().sum()
        mis_val_percent = 100 * df.isnull().sum() / len(df)

        mz_table = pd.concat([zero_val, mis_val, mis_val_percent], axis=1)
        mz_table = mz_table.rename(columns={0: 'Zero Values', 1: 'Missing Values', 2: '% of Total Values'})
        mz_table['Total Zero Missing Values'] = mz_table['Zero Values'] + mz_table['Missing Values']
        mz_table['% Total Zero Missing Values'] = 100 * mz_table['Total Zero Missing Values'] / len(df)
        mz_table['Data Type'] = df.dtypes
        mz_table = mz_table[mz_table['Total Zero Missing Values'] > 0].sort_values('% of Total Values', ascending=False).round(1)

        print(f"Your selected dataframe has {df.shape[1]} columns and {df.shape[0]} rows.\n"
              f"There are {mz_table.shape[0]} columns that have zero or missing values.")

        return mz_table

    def impute_knn(self, df):
        ldf = df.select_dtypes(include=[np.number])
        ldf_putaside = df.select_dtypes(exclude=[np.number])
        cols_nan = ldf.columns[ldf.isna().any()].tolist()
        cols_no_nan = ldf.columns.difference(cols_nan).values

        for col in cols_nan:
            imp_test = ldf[ldf[col].isna()]
            imp_train = ldf.dropna()
            model = KNeighborsRegressor(n_neighbors=5)
            knr = model.fit(imp_train[cols_no_nan], imp_train[col])
            ldf.loc[df[col].isna(), col] = knr.predict(imp_test[cols_no_nan])
        return pd.concat([ldf, ldf_putaside], axis=1)

    def handle_missing_values(self, df, method='mean'):
        if method == 'ffill':
            df.fillna(method='ffill', inplace=True)
        elif method == 'mean':
            for column in df.columns:
                if df[column].dtype == 'float64':
                    df[column].fillna(np.mean(df[column]), inplace=True)
        else:
            raise ValueError("Invalid method. Choose 'ffill' or 'mean'.")
        return df

    def correlation_matrix(self, df):
        return df.corr()

    def data_normalization(self, df):
        return (df - df.mean()) / df.std()

    def preprocess_data(self, df):
        # Step 1: Handle missing and zero values
        mz_table = self.missing_zero_values_table(df)
        if self.impute_method == 'knn':
            df = self.impute_knn(df)
        elif self.impute_method == 'mean':
            df = self.handle_missing_values(df)
        else:
            raise ValueError("Invalid impute_method. Choose 'knn' or 'mean'.")

        # Step 2: Perform data normalization
        if self.normalization:
            df = self.data_normalization(df)

        return df


    #Feature Engineering
    def feature_engineering(self, df):
      df = pd.DataFrame(df)
      # Sorting the DataFrame by Date
      df.sort_values(by='Date', inplace=True)

      # Adding Month and Year in separate columns
      df['Day'] = df.index.day
      df['Month'] = df.index.month
      df['Year'] = df.index.year

      df['Close_Diff'] = df['Adj Close'].diff()

      # Moving averages - different periods
      df['MA200'] = df['Close'].rolling(window=200).mean()
      df['MA100'] = df['Close'].rolling(window=100).mean()
      df['MA50'] = df['Close'].rolling(window=50).mean()
      df['MA26'] = df['Close'].rolling(window=26).mean()
      df['MA20'] = df['Close'].rolling(window=20).mean()
      df['MA12'] = df['Close'].rolling(window=12).mean()
      df['Moving_Average_Crossover'] = (df['MA50'] > df['MA200']).astype(int)

      # SMA Differences - different periods
      df['DIFF-MA200-MA50'] = df['MA200'] - df['MA50']
      df['DIFF-MA200-MA100'] = df['MA200'] - df['MA100']
      df['DIFF-MA200-CLOSE'] = df['MA200'] - df['Close']
      df['DIFF-MA100-CLOSE'] = df['MA100'] - df['Close']
      df['DIFF-MA50-CLOSE'] = df['MA50'] - df['Close']

      # Moving Averages on high, lows, and std - different periods
      df['MA200_low'] = df['Low'].rolling(window=200).min()
      df['MA14_low'] = df['Low'].rolling(window=14).min()
      df['MA200_high'] = df['High'].rolling(window=200).max()
      df['MA14_high'] = df['High'].rolling(window=14).max()
      df['MA20dSTD'] = df['Close'].rolling(window=20).std()

      # Exponential Moving Averages (EMAS) - different periods
      df['EMA12'] = df['Close'].ewm(span=12, adjust=False).mean()
      df['EMA20'] = df['Close'].ewm(span=20, adjust=False).mean()
      df['EMA26'] = df['Close'].ewm(span=26, adjust=False).mean()
      df['EMA100'] = df['Close'].ewm(span=100, adjust=False).mean()
      df['EMA200'] = df['Close'].ewm(span=200, adjust=False).mean()

      # Shifts (one day before and two days before)
      df['close_shift-1'] = df.shift(-1)['Close']
      df['close_shift-2'] = df.shift(-2)['Close']

      # Bollinger Bands
      df['Bollinger_Upper'] = df['MA20'] + (df['MA20dSTD'] * 2)
      df['Bollinger_Lower'] = df['MA20'] - (df['MA20dSTD'] * 2)

      # Relative Strength Index (RSI)
      df['K-ratio'] = 100*((df['Close'] - df['MA14_low']) / (df['MA14_high'] - df['MA14_low']) )
      df['RSI'] = df['K-ratio'].rolling(window=3).mean()

      # Moving Average Convergence/Divergence (MACD)
      df['MACD'] = df['EMA12'] - df['EMA26']
      df['Signal_Line'] = df['MACD'].ewm(span=9, min_periods=1, adjust=False).mean()

      # Replace nas
      nareplace = df.at[df.index.max(), 'Close']
      df.fillna((nareplace), inplace=True)
      df = pd.DataFrame(df)

      return df

#Visualization
class StockDataVisualizer:
    @staticmethod
    def plot_daily_currency_price(df, currency_pair, price_column='Adj Close'):
        if currency_pair not in df.keys():
            print(f"Error: '{currency_pair}' not found in the DataFrame.")
            return

        if price_column not in df[currency_pair].columns:
            print(f"Error: '{price_column}' column not found in the DataFrame for {currency_pair}.")
            return

        fig = go.Figure()
        fig.add_trace(go.Scatter(x=df[currency_pair].index, y=df[currency_pair][price_column], mode='lines', name=f'{currency_pair} - {price_column}', line=dict(color='blue')))

        fig.update_layout(title=f'Visualizing {currency_pair} - {price_column} Prices',
                          xaxis_title='Date',
                          yaxis_title='Price',
                          xaxis_rangeslider_visible=True,
                          legend=dict(font=dict(size=12)),
                          margin=dict(l=50, r=30, t=50, b=30))

        fig.show()

    @staticmethod
    def plot_currency_ohcl_prices(df):
        required_columns = ['Open', 'High', 'Low', 'Close', 'Adj Close']

        missing_columns = [column for column in required_columns if column not in df.columns]
        if missing_columns:
            raise ValueError(f"DataFrame is missing the following required columns: {', '.join(missing_columns)}")

        colors = ['blue', 'green', 'red', 'black', 'brown']

        fig = go.Figure()
        for i, component in enumerate(required_columns):
            fig.add_trace(go.Scatter(x=df.index, y=df[component], mode='lines', name=component, line=dict(color=colors[i])))

        fig.update_layout(title='Visualizing Daily OHCL Prices',
                          xaxis_title='Date',
                          yaxis_title='Price',
                          xaxis_rangeslider_visible=True,
                          legend=dict(font=dict(size=12)),
                          margin=dict(l=50, r=30, t=50, b=30))

        fig.show()

    @staticmethod
    def plot_two_currency(currency_data, pair1, pair2, price_choice):
        ax = plt.figure(figsize=(15, 7))

        ax = currency_data[pair1][price_choice].plot(label=pair1)
        ax2 = currency_data[pair2][price_choice].plot(secondary_y=True, color='g', ax=ax, label=pair2)

        plt.title(f'{pair1} and {pair2} {price_choice} Data', fontsize=16)
        ax.set_xlabel('Year-Month', fontsize=15)
        ax.set_ylabel(f'{price_choice} Prices', fontsize=15)
        ax2.set_ylabel(f'{price_choice} Prices', fontsize=15)
        ax.tick_params(axis='both', labelsize=15)
        h1, l1 = ax.get_legend_handles_labels()
        h2, l2 = ax2.get_legend_handles_labels()
        ax.legend(h1+h2, l1+l2, loc=2, prop={'size': 15})

        plt.show()

    @staticmethod
    def visualize_stock_analysis(df):
        plt.figure(figsize=(12, 6))
        plt.plot(df.index, df['Close'], label='Close', color='b', linewidth=2)
        plt.plot(df.index, df['MA50'], label='50-day MA', color='orange', linestyle='dashed', linewidth=2)
        plt.plot(df.index, df['MA200'], label='200-day MA', color='purple', linestyle='dashed', linewidth=2)
        plt.plot(df.index, df['Bollinger_Upper'], label='Upper Bollinger Band', color='green', linestyle='dotted', linewidth=1)
        plt.plot(df.index, df['Bollinger_Lower'], label='Lower Bollinger Band', color='red', linestyle='dotted', linewidth=1)
        plt.fill_between(df.index, df['Bollinger_Lower'], df['Bollinger_Upper'], color='gray', alpha=0.2)

        plt.title('Stock Price, Moving Averages, Bollinger Bands, and More', fontsize=16)
        plt.xlabel('Date', fontsize=12)
        plt.ylabel('Price', fontsize=12)
        plt.legend()
        plt.grid(True)
        plt.tight_layout()
        plt.show()

    @staticmethod
    def visualize_stock_analysis_interactive(df):
        fig = make_subplots(rows=2, cols=1, shared_xaxes=True, vertical_spacing=0.1)

        fig.add_trace(go.Scatter(x=df.index, y=df['Close'], name='Close', line=dict(color='blue', width=2)))
        fig.add_trace(go.Scatter(x=df.index, y=df['MA50'], name='50-day MA', line=dict(color='orange', width=2, dash='dash')))
        fig.add_trace(go.Scatter(x=df.index, y=df['MA200'], name='200-day MA', line=dict(color='purple', width=2, dash='dash')))

        fig.add_trace(go.Scatter(x=df.index, y=df['Bollinger_Upper'], name='Upper Bollinger Band', line=dict(color='green', width=1, dash='dot')))
        fig.add_trace(go.Scatter(x=df.index, y=df['Bollinger_Lower'], name='Lower Bollinger Band', line=dict(color='red', width=1, dash='dot')))

        fig.add_trace(go.Scatter(x=df.index, y=df['EMA12'], name='EMA12', line=dict(color='cyan', width=1, dash='dash')))
        fig.add_trace(go.Scatter(x=df.index, y=df['EMA26'], name='EMA26', line=dict(color='magenta', width=1, dash='dash')))

        fig.add_trace(go.Scatter(x=df.index, y=df['RSI'], name='RSI', line=dict(color='blue', width=1, dash='dot')))

        fig.update_layout(
            title='Interactive Stock Analysis',
            xaxis_title='Date',
            yaxis_title='Price',
            legend=dict(orientation='h', yanchor='bottom', y=1.02, xanchor='right', x=1),
            template='plotly_white'
        )

        fig.update_xaxes(showspikes=True, spikemode='across', spikesnap='cursor')
        fig.update_yaxes(showspikes=True, spikemode='across', spikesnap='cursor')

        fig.show()

class TimeSeriesAddOns:
    def __init__(self, series):
        self.series = series

    def plot_moving_average(self, window, plot_intervals=False, scale=1.96):
        rolling_mean = self.series.rolling(window=window).mean()

        plt.figure(figsize=(17, 8))
        plt.title('Moving average\n window size = {}'.format(window))
        plt.plot(rolling_mean, 'g', label='Rolling mean trend')

        if plot_intervals:
            mae = mean_absolute_error(self.series[window:], rolling_mean[window:])
            deviation = np.std(self.series[window:] - rolling_mean[window:])
            lower_bound = rolling_mean - (mae + scale * deviation)
            upper_bound = rolling_mean + (mae + scale * deviation)
            plt.plot(upper_bound, 'r--', label='Upper bound / Lower bound')
            plt.plot(lower_bound, 'r--')

        plt.plot(self.series[window:], label='Actual values')
        plt.legend(loc='best')
        plt.grid(True)

    @staticmethod
    def exponential_smoothing(series, alpha):
        result = [series[0]]
        for n in range(1, len(series)):
            result.append(alpha * series[n] + (1 - alpha) * result[n-1])
        return result

    def plot_exponential_smoothing(self, alphas):
        plt.figure(figsize=(17, 8))
        for alpha in alphas:
            plt.plot(self.exponential_smoothing(self.series, alpha), label="Alpha {}".format(alpha))
        plt.plot(self.series.values, "c", label="Actual")
        plt.legend(loc="best")
        plt.axis('tight')
        plt.title("Exponential Smoothing")
        plt.grid(True)

    @staticmethod
    def double_exponential_smoothing(series, alpha, beta):
        result = [series[0]]
        for n in range(1, len(series) + 1):
            if n == 1:
                level, trend = series[0], series[1] - series[0]
            if n >= len(series):
                value = result[-1]
            else:
                value = series[n]
            last_level, level = level, alpha * value + (1 - alpha) * (level + trend)
            trend = beta * (level - last_level) + (1 - beta) * trend
            result.append(level + trend)
        return result

    def plot_double_exponential_smoothing(self, alphas, betas):
        plt.figure(figsize=(17, 8))
        for alpha in alphas:
            for beta in betas:
                plt.plot(self.double_exponential_smoothing(self.series, alpha, beta),
                         label="Alpha {}, beta {}".format(alpha, beta))
        plt.plot(self.series.values, label="Actual")
        plt.legend(loc="best")
        plt.axis('tight')
        plt.title("Double Exponential Smoothing")
        plt.grid(True)


#Analyzing
class TimeSeriesAnalyzer:
    def ts_transform(self, timeseries, model, freq):
        def data_decompose(timeseries, model, freq):
            decomposition = sm.tsa.seasonal_decompose(timeseries, model=model, freq=freq)
            return decomposition

        result = data_decompose(timeseries, model=model, freq=freq)
        ts_trend = result.trend
        ts_seasonal = result.seasonal
        ts_resid = result.resid
        ts_observed = result.observed
        return ts_trend, ts_seasonal, ts_resid, ts_observed

    def plot_decompose(self, ts, ts_trend, ts_seasonal, ts_resid, figsize=(12, 10)):
        plt.figure(figsize=figsize)

        plt.subplot(4, 1, 1)
        plt.plot(ts, label='Original')
        plt.legend(loc='best')
        plt.title('Original Time Series')

        plt.subplot(4, 1, 2)
        plt.plot(ts_trend, label='Trend')
        plt.legend(loc='best')
        plt.title('Trend Component')

        plt.subplot(4, 1, 3)
        plt.plot(ts_seasonal, label='Seasonality')
        plt.legend(loc='best')
        plt.title('Seasonal Component')

        plt.subplot(4, 1, 4)
        plt.plot(ts_resid, label='Residuals')
        plt.legend(loc='best')
        plt.title('Residual Component')

        plt.tight_layout()
        plt.show()

#Performing the Seasonal ARIMA
class TimeSeriesOperations:
    def __init__(self):
        pass

    def difference(self, dataset, interval=1):
        diff = list()
        for i in range(interval, len(dataset)):
            value = dataset[i] - dataset[i - interval]
            if not np.isnan(value):
                diff.append(value)
        return pd.Series(diff)

    def inverse_difference(self, history, yhat, interval=1):
        return yhat + history[-interval]

    def tsplot(self, y, lags=None, figsize=(12, 7), style='bmh'):
        if not isinstance(y, pd.Series):
            y = pd.Series(y)

        with plt.style.context(style='bmh'):
            fig = plt.figure(figsize=figsize)
            layout = (2, 2)
            ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)
            acf_ax = plt.subplot2grid(layout, (1, 0))
            pacf_ax = plt.subplot2grid(layout, (1, 1))

            y.plot(ax=ts_ax)
            p_value = sm.tsa.stattools.adfuller(y)[1]
            ts_ax.set_title('Time Series Analysis Plots\n Dickey-Fuller: p={0:.5f}'.format(p_value))
            sm.graphics.tsa.plot_acf(y, lags=lags, ax=acf_ax)
            sm.graphics.tsa.plot_pacf(y, lags=lags, ax=pacf_ax)
            plt.tight_layout()

    def test_stationarity(self, timeseries, window=12, cutoff=0.01):
        # Plot rolling statistics
        rolmean = timeseries.rolling(window).mean()
        rolstd = timeseries.rolling(window).std()
        fig = plt.figure(figsize=(12, 8))
        orig = plt.plot(timeseries, color='blue', label='Original')
        mean = plt.plot(rolmean, color='red', label='Rolling Mean')
        std = plt.plot(rolstd, color='black', label='Rolling Std')
        plt.legend(loc='best')
        plt.title('Rolling Mean & Standard Deviation')
        plt.show()

        # Perform Dickey-Fuller test
        print('Results of Dickey-Fuller Test:')
        dftest = adfuller(timeseries, autolag='AIC', maxlag=20)
        dfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])
        for key, value in dftest[4].items():
            dfoutput['Critical Value (%s)' % key] = value
        pvalue = dftest[1]
        if pvalue < cutoff:
            print('p-value = %.4f. The series is likely stationary.' % pvalue)
        else:
            print('p-value = %.4f. The series is likely non-stationary.' % pvalue)
        print(dfoutput)

        # Perform KPSS test
        print('\nResults of KPSS Test:')
        stats, p, lags, critical_values = kpss(timeseries, 'ct')
        kpss_output = pd.Series(stats, index=['KPSS Test Statistic'])
        kpss_output['p-value'] = p
        kpss_output['Lags Used'] = lags
        for key, value in critical_values.items():
            kpss_output['Critical Value (%s)' % key] = value
        if p < cutoff:
            print('p-value = %.4f. The series is likely non-stationary.' % p)
        else:
            print('p-value = %.4f. The series is likely stationary.' % p)
        print(kpss_output)

    def train_test_split(self, data, ratio):
        train = data[:int(ratio*(len(data)))]
        test = data[int(ratio*(len(data))):]
        return train, test

    def plot_train_test(self, train, test):
        plt.figure(figsize=(10,6))
        plt.grid(True)
        plt.xlabel('Dates')
        plt.ylabel('Closing Prices')
        plt.plot(train, 'Maroon', label='Train data')
        plt.plot(test, 'Blue', label='Test data')
        plt.legend()
        plt.show()

    def plot_acf_pacf(self, data, nlags=20):
        lag_acf = acf(data, nlags=nlags)
        lag_pacf = pacf(data, nlags=nlags, method='ols')

        # ACF
        plt.figure(figsize=(22, 7))
        plt.subplot(121)
        plt.plot(lag_acf)
        plt.axhline(y=0, linestyle='--', color='gray')
        plt.axhline(y=-1.96 / np.sqrt(len(data)), linestyle='--', color='gray')
        plt.axhline(y=1.96 / np.sqrt(len(data)), linestyle='--', color='gray')
        plt.title('Autocorrelation Function')

        # PACF
        plt.subplot(122)
        plt.plot(lag_pacf)
        plt.axhline(y=0, linestyle='--', color='gray')
        plt.axhline(y=-1.96 / np.sqrt(len(data)), linestyle='--', color='gray')
        plt.axhline(y=1.96 / np.sqrt(len(data)), linestyle='--', color='gray')
        plt.title('Partial Autocorrelation Function')
        plt.tight_layout()

    def plot_autocorrelation(self, data, nlags=20):
        # Calculate ACF and PACF
        lag_acf = acf(data, nlags=nlags)
        lag_pacf = pacf(data, nlags=nlags, method='ols')

        # Plot ACF
        plt.figure(figsize=(22,7))
        plt.subplot(121)
        plot_acf(data, ax=plt.gca(), lags=nlags)

        # Plot PACF
        plt.subplot(122)
        plot_pacf(data, ax=plt.gca(), lags=nlags)

        plt.tight_layout()
        plt.show()


    @classmethod
    def arimamodel(cls, timeseries):
        automodel = pmd.auto_arima(timeseries, start_p=1, start_q=1,
                                    test='adf',
                                    max_p=5, max_q=5,
                                    m=1,
                                    d=1,
                                    seasonal=False,
                                    start_P=0,
                                    D=None,
                                    trace=True,
                                    error_action='ignore',
                                    suppress_warnings=True,
                                    stepwise=True)
        return automodel

    @classmethod
    def arimamodel2(cls, timeseries):
        stepwise_model = pmd.auto_arima(timeseries,
                                   start_p=1, start_q=1,
                                   max_p=3, max_q=3,
                                   m=12,  # Monthly seasonality (if your data is monthly)
                                   d=1,   # Non-seasonal differencing order
                                   D=1,   # Seasonal differencing order
                                   seasonal=True,  # Include seasonal components
                                   trace=True,
                                   error_action='ignore',
                                   suppress_warnings=True,
                                   stepwise=True)
        return stepwise_model

    @classmethod
    def plotarima(cls, n_periods, timeseries, automodel):
        fc, confint = automodel.predict(n_periods=n_periods,
                                    return_conf_int=True)
        fc_ind = pd.date_range(timeseries.index[-1], periods=n_periods, freq="W")
        fc_series = pd.Series(fc, index=fc_ind)
        lower_series = pd.Series(confint[:, 0], index=fc_ind)
        upper_series = pd.Series(confint[:, 1], index=fc_ind)

        plt.figure(figsize=(10, 6))
        plt.plot(timeseries, label="Past", color="blue")
        plt.plot(fc_series, color="red", label="Forecast")
        plt.fill_between(fc_ind, lower_series, upper_series, color="gray", alpha=0.3, label="95% Confidence Interval")
        plt.xlabel("Date")
        plt.ylabel(timeseries.name)
        plt.legend(loc="upper left")
        plt.show()

    def tune_arima_hyperparameters(self, train):
        # Use auto_arima to find the best ARIMA hyperparameters
        automodel = auto_arima(train, start_p=1, start_q=1, max_p=5, max_q=5,
                               seasonal=False, stepwise=True, suppress_warnings=True)

        return automodel.order

class TimeSeriesForecasting:
    def arima_prediction(self, data, order):
        try:
            # Convert 'data' to a pandas Series if it's not already
            if not isinstance(data, pd.Series):
                data = pd.Series(data)

            # Fit ARIMA model
            model = ARIMA(data, order=order)
            results = model.fit()

            # Make predictions for the next 20 time steps
            forecast = results.forecast(steps=20)

            # Create plot
            plt.figure(figsize=(12, 6))
            plt.plot(data, label="Original Data", color="blue")
            plt.plot(np.arange(len(data), len(data) + 20), forecast, label="Forecast", color="red")
            plt.fill_between(np.arange(len(data), len(data) + 20), conf_int[:, 0], conf_int[:, 1], color="gray", alpha=0.3, label="95% Confidence Interval")
            plt.xlabel("Time Steps")
            plt.ylabel(data.name)
            plt.title("ARIMA Prediction Visualization", size=20)
            plt.legend()
            plt.show()

            return forecast

        except Exception as e:
            print("An error occurred:", e)
            return None

    def arima_prediction1(self, data, order, n_periods=20):
        # Fit ARIMA model
        model = sm.tsa.ARIMA(data, order=order)
        results = model.fit()

        # Forecast
        fc = results.predict(start=len(data), end=len(data) + n_periods - 1)
        #forecast, stderr, conf_int = results.forecast(steps=20, alpha=0.05)

        # Plot forecast
        plt.figure(figsize=(10, 6))
        plt.plot(data)
        plt.plot(fc, color="red")
        plt.xlabel("date")
        plt.ylabel(data.name)
        plt.title("ARIMA prediction Visualization", size=20)

        # Save plot
        plt.savefig("arima_forecast.png")

        # Show plot
        plt.show()

    def combine_forecasts(self, forecast1, forecast2, test_index):
        # Create DataFrames for forecast1 and forecast2 with appropriate indices
        future_forecast1 = pd.DataFrame(forecast1, index=test_index, columns=['Prediction1'])
        future_forecast2 = pd.DataFrame(forecast2, index=test_index, columns=['Prediction2'])

        # Combine the forecasts into a single DataFrame named 'Results'
        results = pd.concat([future_forecast1, future_forecast2], axis=1)
        return results

    def combine_forecasts1(self, forecast1, forecast2, train):
        # Check if forecast1 and train are not None
        if forecast1 is not None and train is not None:
            # Convert forecast1 to a pandas Series with appropriate indices
            forecast1_series = pd.Series(forecast1, index=train.index[-len(forecast1):])

            # Check if forecast2 is not None
            if forecast2 is not None:
                # Create a new date range for forecast2 (assuming it's 20 periods ahead)
                forecast2_date_range = pd.date_range(start=train.index[-1], periods=len(forecast2), freq=train.index.inferred_freq)

                # Convert forecast2 to a pandas Series with the new date range
                forecast2_series = pd.Series(forecast2, index=forecast2_date_range)

                # Create a DataFrame by combining the Series
                combined_forecasts = pd.concat([forecast1_series, forecast2_series], axis=1)

                return combined_forecasts
            else:
                print("Forecast2 is None. Cannot combine forecasts.")
        else:
            print("Forecast1 or Train is None. Cannot create combined DataFrame.")

        return None
def tune_arima_hyperparameters(train):
    # Define the parameter grid for hyperparameter tuning
    p_values = range(0, 4)  # Replace with your desired range
    d_values = range(0, 2)  # Replace with your desired range
    q_values = range(0, 4)  # Replace with your desired range
    seasonal_values = [0]  # Non-seasonal ARIMA
    pdq_parameters = list(ParameterGrid({'p': p_values, 'd': d_values, 'q': q_values}))

    best_aic = float("inf")
    best_order = None

    for pdq in pdq_parameters:
        for seasonal_order in seasonal_values:
            order = (pdq['p'], pdq['d'], pdq['q'])
            seasonal_order = (0, 0, 0, seasonal_order)
            model = SARIMAX(train, order=order, seasonal_order=seasonal_order, enforce_stationarity=False, enforce_invertibility=False)
            results = model.fit(disp=False)

            if results.aic < best_aic:
                best_aic = results.aic
                best_order = order

    return best_order


#Accuracy Metrics
class Metrics:
    def regression_metrics(self, y_test, y_pred):
        if len(y_test) != len(y_pred):
            raise ValueError("y_test and y_pred must have the same length.")

        metrics = {}

        # Root mean squared error
        RMSE = np.sqrt(mean_squared_error(y_test, y_pred))
        metrics['Root Mean Squared Error (RMSE)'] = np.round(RMSE, 2)

        # Median absolute error
        MAE = mean_absolute_error(y_test, y_pred)
        metrics['Median Absolute Error (MAE)'] = np.round(MAE, 2)

        # Mean absolute percentage error (MAPE)
        if 0 in y_test:
            metrics['Mean Absolute Percentage Error (MAPE)'] = 'Cannot be calculated (division by zero)'
        else:
            MAPE = np.mean((np.abs(np.subtract(y_test, y_pred) / y_test))) * 100
            metrics['Mean Absolute Percentage Error (MAPE)'] = np.round(MAPE, 2)

        # Median Absolute Percentage Error
        if 0 in y_test:
            metrics['Median Absolute Percentage Error (MDAPE)'] = 'Cannot be calculated (division by zero)'
        else:
            MDAPE = np.median((np.abs(np.subtract(y_test, y_pred) / y_test))) * 100
            metrics['Median Absolute Percentage Error (MDAPE)'] = np.round(MDAPE, 2)

        # Mean squared error
        MSE = mean_squared_error(y_test, y_pred)
        metrics['Mean Squared Error (MSE)'] = np.round(MSE, 2)

        # R-squared value
        R2 = r2_score(y_test, y_pred)
        metrics['R-squared (R2)'] = np.round(R2, 2)

        # Interpretation based on the results
        if R2 >= 0.7:
            interpretation = "The model explains a significant portion of the variance in the data (R-squared >= 0.7)."
        elif R2 >= 0.5:
            interpretation = "The model explains a moderate amount of variance in the data (0.5 <= R-squared < 0.7)."
        else:
            interpretation = "The model has a weak performance in explaining the variance in the data (R-squared < 0.5)."

        if RMSE <= 10:
            interpretation += " Additionally, the RMSE suggests that the model's predictions are relatively accurate."
        else:
            interpretation += " The RMSE indicates that the model's predictions might have larger errors."

        metrics['Interpretation'] = interpretation

        # Print formatted regression metrics
        regression_table = []
        for metric, value in metrics.items():
            regression_table.append([metric, value])

        print("Regression Metrics:")
        print(tabulate(regression_table, headers=["Metric", "Value"], tablefmt="fancy_grid"))

        return metrics

    def classification_metrics(self, y_test, y_pred):
        if len(y_test) != len(y_pred):
            raise ValueError("y_test and y_pred must have the same length.")

        unique_labels = np.unique(y_test)
        num_classes = len(unique_labels)
        if num_classes == 2:
            binary_classification = True
        else:
            binary_classification = False

        metrics = {}

        if binary_classification:
            fpr, tpr, _ = roc_curve(y_test, y_pred)
            roc_auc = auc(fpr, tpr)

            metrics['ROC AUC'] = np.round(roc_auc, 2)

        cm = confusion_matrix(y_test, y_pred)
        metrics['Confusion Matrix'] = cm.tolist()

        accuracy = accuracy_score(y_test, y_pred)
        metrics['Accuracy'] = np.round(accuracy, 2)

        if binary_classification:
            precision = precision_score(y_test, y_pred)
            metrics['Precision'] = np.round(precision, 2)

        if binary_classification:
            recall = recall_score(y_test, y_pred)
            metrics['Recall (Sensitivity)'] = np.round(recall, 2)

        if binary_classification:
            F1 = f1_score(y_test, y_pred)
            metrics['F1-score'] = np.round(F1, 2)

        MCC = matthews_corrcoef(y_test, y_pred)
        metrics['Matthews correlation coefficient'] = np.round(MCC, 2)

        if binary_classification:
            if roc_auc >= 0.8:
                interpretation = "The model shows excellent discrimination ability (ROC AUC >= 0.8)."
            elif roc_auc >= 0.7:
                interpretation = "The model exhibits good discrimination ability (0.7 <= ROC AUC < 0.8)."
            else:
                interpretation = "The model's discrimination ability is moderate (ROC AUC < 0.7)."

            if F1 >= 0.7:
                interpretation += " Additionally, the F1-score indicates that the model's predictions have good balance between precision and recall."
            else:
                interpretation += " The F1-score suggests that the model's predictions may have imbalanced precision and recall."

        else:
            interpretation = "This is a multiclass classification problem. Interpretation based on metrics is not provided for multiclass."

        metrics['Interpretation'] = interpretation

        # Print formatted classification metrics
        classification_table = []
        for metric, value in metrics.items():
            if isinstance(value, list):
                classification_table.append([metric, ""])
                for row in value:
                    classification_table.append(["", row])
            else:
                classification_table.append([metric, value])

        print("\nClassification Metrics:")
        print(tabulate(classification_table, headers=["Metric", "Value"], tablefmt="fancy_grid"))

        return metrics

    def mean_absolute_scaled_error(self, y_true, y_pred):
        naive_forecast = y_true.shift(1)
        naive_errors = np.abs(y_true - naive_forecast)
        mase = np.mean(np.abs(y_true - y_pred) / naive_errors)
        return mase

    def theils_u_statistic(self, y_true, y_pred, y_ref):
        numerator = np.sqrt(np.mean((y_true - y_pred)**2))
        denominator = np.sqrt(np.mean((y_ref - y_true)**2))
        theils_u = numerator / denominator
        return theils_u



"""##Additionals"""

def search_results(query):
    links = []
    try:
        for result in search(query, num=10, stop=10, pause=2):
            links.append(result)
        return links
    except Exception as e:
        print(f"An error occurred: {e}")
        return []

def plot_moving_average(series, window, plot_intervals=False, scale=1.96):

    rolling_mean = series.rolling(window=window).mean()

    plt.figure(figsize=(17,8))
    plt.title('Moving average\n window size = {}'.format(window))
    plt.plot(rolling_mean, 'g', label='Rolling mean trend')

    #Plot confidence intervals for smoothed values
    if plot_intervals:
        mae = mean_absolute_error(series[window:], rolling_mean[window:])
        deviation = np.std(series[window:] - rolling_mean[window:])
        lower_bound = rolling_mean - (mae + scale * deviation)
        upper_bound = rolling_mean + (mae + scale * deviation)
        plt.plot(upper_bound, 'r--', label='Upper bound / Lower bound')
        plt.plot(lower_bound, 'r--')

    plt.plot(series[window:], label='Actual values')
    plt.legend(loc='best')
    plt.grid(True)

def exponential_smoothing(series, alpha):
    result = [series[0]]
    for n in range(1, len(series)):
        result.append(alpha * series[n] + (1 - alpha) * result[n-1])
    return result

def plot_exponential_smoothing(series, alphas):

    plt.figure(figsize=(17, 8))
    for alpha in alphas:
        plt.plot(exponential_smoothing(series, alpha), label="Alpha {}".format(alpha))
    plt.plot(series.values, "c", label = "Actual")
    plt.legend(loc="best")
    plt.axis('tight')
    plt.title("Exponential Smoothing")
    plt.grid(True);

def double_exponential_smoothing(series, alpha, beta):

    result = [series[0]]
    for n in range(1, len(series)+1):
        if n == 1:
            level, trend = series[0], series[1] - series[0]
        if n >= len(series): # forecasting
            value = result[-1]
        else:
            value = series[n]
        last_level, level = level, alpha * value + (1 - alpha) * (level + trend)
        trend = beta * (level - last_level) + (1 - beta) * trend
        result.append(level + trend)
    return result

def plot_double_exponential_smoothing(series, alphas, betas):

    plt.figure(figsize=(17, 8))
    for alpha in alphas:
        for beta in betas:
            plt.plot(double_exponential_smoothing(series, alpha, beta), label="Alpha {}, beta {}".format(alpha, beta))
    plt.plot(series.values, label = "Actual")
    plt.legend(loc="best")
    plt.axis('tight')
    plt.title("Double Exponential Smoothing")
    plt.grid(True)

"""*SnooG in Action*

#SnooG In Action
"""

def main():
    root_directory = "/content/drive/"
    project_folder_name = "SnooG"
    onono = Onono()
    onono.search_results()
    onono.create_and_set_working_directory(root_directory, project_folder_name)

    tickers = ['EURGBP=X', 'EURUSD=X', 'GBPUSD=X', 'NZDUSD=X', 'USDJPY=X', 'GBPJPY=X', 'EURJPY=X', 'AUDUSD=X', 'USDCHF=X']
    start_date = '2019-01-01'
    end_date = date.today().strftime("%Y-%m-%d")
    interval = '1d'

    currency_processor = CurrencyDataProcessor(tickers)
    currency_data = currency_processor.fetch_currency_data(start_date=start_date, end_date=end_date, interval=interval)

    preprocessor = DataPreprocessor(impute_method='knn', normalization=True)

    currency_pair = input("Enter Currency Pair (e.g., EURGBP): ").strip()

    if currency_pair in currency_processor.currency_data:
        data = currency_processor.currency_data[currency_pair]
        preprocessed_data = preprocessor.preprocess_data(data)
        engineered_data = preprocessor.feature_engineering(preprocessed_data)
        engineered_data.reset_index(drop=True, inplace=True)
        print("Preprocessed and engineered data:")
        print(engineered_data)
    else:
        print(f"Error: '{currency_pair}' is not a valid currency pair.")

    download_link = onono.create_download_link(engineered_data)
    print("Download link:", download_link)

    currency_choice = input("Enter Currency Pair: ").strip()
    pair1 = currency_choice
    pair2 = input("Enter Currency Pair 2 to compare (e.g. GBPUSD): ").strip()
    price_choice = input("Enter Price Type (e.g., Adj Close, High, Low, Close, Open): ").strip()

    visualizer = StockDataVisualizer()

    if currency_choice in currency_data:
        visualizer.plot_daily_currency_price(currency_data, currency_choice, price_choice)
    else:
        print("Error: Invalid currency pair choice. Please choose from the available currency pairs.")

    visualizer.plot_currency_ohcl_prices(engineered_data)
    visualizer.visualize_stock_analysis(engineered_data)
    visualizer.visualize_stock_analysis_interactive(engineered_data)

    """
    # Create an instance of TimeSeriesAddOns class
    ts_add_ons = TimeSeriesAddOns(engineered_data.High)
    ts_add_ons.plot_moving_average(window=7, plot_intervals=True)
    ts_add_ons.plot_exponential_smoothing(alphas=[0.2, 0.5, 0.8])
    ts_add_ons.plot_double_exponential_smoothing(engineered_data.High, alphas=[0.2, 0.5, 0.8], betas=[0.2, 0.5, 0.8])
    """
    # Create an instance of TimeSeriesOperations
    operations = TimeSeriesOperations()

    # Perform differencing on the High column
    data_diff = operations.difference(engineered_data.High, interval=1)

    # Visualize the time series and its autocorrelation/partial autocorrelation plots
    operations.tsplot(data_diff, lags=30)

    # Test the stationarity of the differenced time series
    operations.test_stationarity(data_diff, window=12, cutoff=0.01)

    # Perform train-test split on the data
    train, test = operations.train_test_split(data_diff, 0.7)

    # Visualize the train and validation sets
    operations.plot_train_test(train, test)

    # Plot the autocorrelation and partial autocorrelation plots
    operations.plot_acf_pacf(data_diff, nlags=20)

    # Plot the autocorrelation and partial autocorrelation plots
    operations.plot_autocorrelation(data_diff, nlags=20)

        # Fit the Seasonal ARIMA model
    #automodel = operations.tune_arima_hyperparameters(train)  # Get the best ARIMA order
    #automodel.fit(train)  # Fit the model with the best order

    # Fit the Seasonal ARIMA model
    automodel = operations.arimamodel(train)
    stepwise = operations.arimamodel2(train)

    # Plot the ARIMA forecast
    operations.plotarima(n_periods=100, timeseries=train, automodel=automodel)
    operations.plotarima(n_periods=100, timeseries=train, automodel=stepwise)

    #Model Selection and Hyperparameter Tuning: Implement hyperparameter tuning for ARIMA
    best_arima_order = operations.tune_arima_hyperparameters(train)

    # Create an instance of the TimeSeriesForecasting class
    forecasting = TimeSeriesForecasting()

    order = (1, 1, 1)
    forecast = forecasting.arima_prediction(train, order)

    # Check if forecast is not None before calculating metrics
    if forecast is not None:
        # Create an instance of Metrics
        metrics = Metrics()

        # Regression metrics
        regression_metrics = metrics.regression_metrics(test, forecast)

        mase_arima = metrics.mean_absolute_scaled_error(test, arima_forecast)
        theils_u_lstm = metrics.theils_u_statistic(test, lstm_forecast)
        theils_u_ensemble = metrics.theils_u_statistic(test, ensemble_forecast)
        theils_u_bsts = metrics.theils_u_statistic(test, bsts_forecast)

        print("Regression Metrics:")
        print(tabulate(regression_metrics.items(), tablefmt="grid"))

        # Classification metrics (Note: You can't directly use classification metrics for regression predictions)
        # classification_metrics = metrics.classification_metrics(test, forecast)
        # print("Classification Metrics:")
        # print(tabulate(classification_metrics.items(), tablefmt="grid"))
    else:
        print("Forecasting did not produce valid results. Metrics calculation skipped.")


if __name__ == "__main__":
    main()

"""#Suggessions for Improvements"""

class LSTMForecasting:
    def __init__(self, units=50, epochs=50, batch_size=32):
        self.units = units
        self.epochs = epochs
        self.batch_size = batch_size

    def forecast(self, train, test):
        # Preprocess the data (convert to sequences, scale, etc.)
        train_sequence, test_sequence = data_diff(train, test)

        # Build the LSTM model
        model = Sequential()
        model.add(LSTM(units=self.units, activation='relu', input_shape=(train_sequence.shape[1], train_sequence.shape[2])))
        model.add(Dense(1))
        model.compile(optimizer='adam', loss='mean_squared_error')

        # Train the model
        model.fit(train_sequence, epochs=self.epochs, batch_size=self.batch_size, verbose=0)

        # Make predictions
        lstm_forecast = model.predict(test_sequence)

        return lstm_forecast

# Fit the Seasonal ARIMA model
automodel = operations.tune_arima_hyperparameters(train)  # Get the best ARIMA order
automodel.fit(train)  # Fit the model with the best order

from statsmodels.tsa.statespace.sarimax import SARIMAX
from sklearn.model_selection import ParameterGrid

def tune_arima_hyperparameters(train):
    # Define the parameter grid for hyperparameter tuning
    p_values = range(0, 4)  # Replace with your desired range
    d_values = range(0, 2)  # Replace with your desired range
    q_values = range(0, 4)  # Replace with your desired range
    seasonal_values = [0]  # Non-seasonal ARIMA
    pdq_parameters = list(ParameterGrid({'p': p_values, 'd': d_values, 'q': q_values}))

    best_aic = float("inf")
    best_order = None

    for pdq in pdq_parameters:
        for seasonal_order in seasonal_values:
            order = (pdq['p'], pdq['d'], pdq['q'])
            seasonal_order = (0, 0, 0, seasonal_order)
            model = SARIMAX(train, order=order, seasonal_order=seasonal_order, enforce_stationarity=False, enforce_invertibility=False)
            results = model.fit(disp=False)

            if results.aic < best_aic:
                best_aic = results.aic
                best_order = order

    return best_order

def main():
    # ... (Your existing code)

    # Fit the Seasonal ARIMA model
    best_order = tune_arima_hyperparameters(train)  # Get the best ARIMA order
    automodel = SARIMAX(train, order=best_order, seasonal_order=(0, 0, 0, 0), enforce_stationarity=False, enforce_invertibility=False)
    results = automodel.fit()

    # Plot the ARIMA forecast
    operations.plotarima(n_periods=100, timeseries=train, automodel=results)

    # ... (Your existing code)

if __name__ == "__main__":
    main()

"""#Sentimental Analysis"""





# Import the necessary classes
from nltk.sentiment import SentimentIntensityAnalyzer
import pandas as pd

class SentimentAnalyzer:
    def __init__(self):
        nltk.download('vader_lexicon')
        self.sid = SentimentIntensityAnalyzer()
        self.sentiment_data = pd.DataFrame(columns=['Date', 'Text', 'Sentiment', 'Score'])

    def analyze_sentiment(self, date, text):
        sentiment_scores = self.sid.polarity_scores(text)
        sentiment_label = self.label_sentiment(sentiment_scores)
        new_row = {
            'Date': date,
            'Text': text,
            'Sentiment': sentiment_label,
            'Score': sentiment_scores['compound']
        }
        self.sentiment_data = pd.concat([self.sentiment_data, pd.DataFrame([new_row])], ignore_index=True)

    def label_sentiment(self, sentiment_scores):
        compound_score = sentiment_scores['compound']
        if compound_score >= 0.05:
            return 'Positive'
        elif compound_score <= -0.05:
            return 'Negative'
        else:
            return 'Neutral'

    def visualize_sentiment(self):
        # Visualize sentiment scores over time or based on specific events
        # You can use libraries like matplotlib or seaborn to create plots or graphs

        # Example code for plotting sentiment scores
        import matplotlib.pyplot as plt

        dates = self.sentiment_data['Date']
        compound_scores = self.sentiment_data['Score']

        plt.plot(dates, compound_scores)
        plt.xlabel('Date')
        plt.ylabel('Sentiment Score')
        plt.title('Sentiment Analysis of Forex Stocks')
        plt.show()

# Example usage
analyzer = SentimentAnalyzer()

# Analyze sentiment of some sample text data
analyzer.analyze_sentiment("2023-08-29", "The stock market is booming!")
analyzer.analyze_sentiment("2023-08-30", "Investors are worried about the trade tensions impacting the forex market.")

# Visualize sentiment scores
analyzer.visualize_sentiment()

"""#Recurrent Neural Networks(RNN)

## Feature Selection and Scaling
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt


class RNNModel:
    def __init__(self, data, target_column, window_size=10):
        self.data = data
        self.target_column = target_column
        self.window_size = window_size
        self.train_data_scaled = None
        self.test_data_scaled = None
        self.model = None

    def train_test_split(self, data, ratio):
        train = data[:int(ratio*(len(data)))]
        test = data[int(ratio*(len(data))):]
        return train, test

    def prepare_data(self):
        # Split data into train and test sets
        train_size = int(len(self.data) * 0.7)
        train_data = self.data.iloc[:train_size]
        test_data = self.data.iloc[train_size:]

        # Scale the data
        scaler = MinMaxScaler(feature_range=(0, 1))
        self.train_data_scaled = scaler.fit_transform(train_data)
        self.test_data_scaled = scaler.transform(test_data)

    def build_model(self):
        self.model = Sequential()
        self.model.add(LSTM(units=50, return_sequences=True, input_shape=(self.window_size, len(self.data.columns))))
        self.model.add(LSTM(units=50))
        self.model.add(Dense(units=1))
        self.model.compile(optimizer='adam', loss='mean_squared_error')

    def train_model(self, epochs=50):
        X_train, y_train = self.create_sequences(self.train_data_scaled)
        self.model.fit(X_train, y_train, epochs=epochs, batch_size=32, verbose=2)

    def create_sequences(self, data):
        X, y = [], []
        for i in range(len(data) - self.window_size):
            X.append(data[i:i+self.window_size])
            y.append(data[i+self.window_size])
        return np.array(X), np.array(y)

    def predict(self):
        X_test, y_test = self.create_sequences(self.test_data_scaled)
        y_pred = self.model.predict(X_test)
        y_pred = y_pred.reshape(-1)
        return y_pred, y_test

    def evaluate(self, y_pred, y_test):
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        print(f"Root Mean Squared Error: {rmse}")

if __name__ == "__main__":
    # Choose the target column for prediction
    target_column = "Close"

    # Create an instance of RNNModel
    rnn_model = RNNModel(currency_data, target_column, window_size=10)

    # Prepare the data
    rnn_model.prepare_data()

    # Build and train the RNN model
    rnn_model.build_model()
    rnn_model.train_model(epochs=50)

    # Make predictions and evaluate the model
    predictions, y_test = rnn_model.predict()
    rnn_model.evaluate(predictions, y_test)

"""Create the dataset with features and filter

## Data Scaling
"""

class RNNModel:
    def __init__(self, data, target_column, window_size=10):
        self.data = data
        self.target_column = target_column
        self.window_size = window_size
        self.train_data_scaled = None
        self.test_data_scaled = None
        self.model = None

    def prepare_data(self, train_ratio=0.7):
        self.train_data_scaled, self.test_data_scaled = self.train_test_split(self.data, train_ratio)
        # You might want to further preprocess your data here

    def train_test_split(self, data, ratio):
        train_size = int(ratio * len(data))
        train = data[:train_size]
        test = data[train_size:]
        return train, test

    def build_model(self):
        self.model = Sequential()
        self.model.add(LSTM(units=50, return_sequences=True, input_shape=(self.window_size, len(self.data.columns))))
        self.model.add(LSTM(units=50))
        self.model.add(Dense(units=1))
        self.model.compile(optimizer='adam', loss='mean_squared_error')

    def train_model(self, epochs=50, batch_size=32):
        X_train, y_train = self.create_sequences(self.train_data_scaled)
        self.model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=2)

    def create_sequences(self, data):
        X, y = [], []
        for i in range(len(data) - self.window_size):
            X.append(data[i:i+self.window_size])
            y.append(data[i+self.window_size])
        return np.array(X), np.array(y)

    def predict(self):
        X_test, y_test = self.create_sequences(self.test_data_scaled)
        y_pred = self.model.predict(X_test)
        y_pred = y_pred.reshape(-1)
        return y_pred, y_test

    def evaluate(self, y_pred, y_test):
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        print(f"Root Mean Squared Error: {rmse}")

if __name__ == "__main__":
    # Choose the target column for prediction
    target_column = "Close"

    # Create an instance of RNNModel
    rnn_model = RNNModel(currency_data, target_column, window_size=10)

    # Prepare the data
    rnn_model.prepare_data()

    # Build and train the RNN model
    rnn_model.build_model()
    rnn_model.train_model(epochs=50, batch_size=32)

    # Make predictions and evaluate the model
    predictions, y_test = rnn_model.predict()
    rnn_model.evaluate(predictions, y_test)

# Get the number of rows in the data
nrows = data_filtered.shape[0]

# Convert the data to numpy values
np_data_unscaled = np.array(data_filtered)
np_data = np.reshape(np_data_unscaled, (nrows, -1))
#print(np_data.shape)

# Transform the data by scaling each feature to a range between 0 and 1
scaler = MinMaxScaler()
np_data_scaled = scaler.fit_transform(np_data_unscaled)

# Creating a separate scaler that works on a single column for scaling predictions
scaler_pred = MinMaxScaler()
df_Close = pd.DataFrame(data_filtered_ext[col])
np_Close_scaled = scaler_pred.fit_transform(df_Close)

"""## Transforming the Data"""

# Set the sequence length - this is the timeframe used to make a single prediction
sequence_length = 50

# Prediction Index
index_Close = data.columns.get_loc("Close")

# Split the training data into train and train data sets
# As a first step, we get the number of rows to train the model on 80% of the data
train_data_len = math.ceil(np_data_scaled.shape[0] * 0.8)

# Create the training and test data
train2 = np_data_scaled[0:train_data_len, :]
test2 = np_data_scaled[train_data_len - sequence_length:, :]

#Splitted Data
#train2, test2 = train_test_split(data_diff, 0.7)

# The RNN needs data with the format of [samples, time steps, features]
# Here, we create N samples, sequence_length time steps per sample, and 6 features
def partition_dataset(sequence_length, data):
    x, y = [], []
    data_len = data.shape[0]
    for i in range(sequence_length, data_len):
        x.append(data[i-sequence_length:i,:]) #contains sequence_length values 0-sequence_length * columsn
        y.append(data[i, index_Close]) #contains the prediction values for validation,  for single-step prediction

    # Convert the x and y to numpy arrays
    x = np.array(x)
    y = np.array(y)
    return x, y

# Generate training data and test data
x_train, y_train = partition_dataset(sequence_length, train2)
x_test, y_test = partition_dataset(sequence_length, test2)

# Print the shapes: the result is: (rows, training_sequence, features) (prediction value, )
print(x_train.shape, y_train.shape)
print(x_test.shape, y_test.shape)

# Validate that the prediction value and the input match up
# The last close price of the second input sample should equal the first prediction value
print(x_train[1][sequence_length-1][index_Close])
print(y_train[0])

"""## Train the Multivariate Prediction Model"""

# Configure the neural network model
model = Sequential()

# Model with n_neurons = inputshape Timestamps, each with x_train.shape[2] variables
n_neurons = x_train.shape[1] * x_train.shape[2]
print(n_neurons, x_train.shape[1], x_train.shape[2])
model.add(LSTM(n_neurons, return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2])))
model.add(LSTM(n_neurons, return_sequences=False))
model.add(Dense(5))
model.add(Dense(1))

# Compile the model
model.compile(optimizer='adam', loss='mse')

"""Training the model"""

# Training the model
epochs = 50
batch_size = 16
early_stop = EarlyStopping(monitor='loss', patience=5, verbose=1)
history = model.fit(x_train, y_train,
                    batch_size=batch_size,
                    epochs=epochs,
                    validation_data=(x_test, y_test)
                   )

                    #callbacks=[early_stop])

"""Loss Values Plot"""

# Plot training & validation loss values
fig, ax = plt.subplots(figsize=(16, 5), sharex=True)
sns.lineplot(data=history.history["loss"])
plt.title("Model loss")
plt.ylabel("Loss")
plt.xlabel("Epoch")
ax.xaxis.set_major_locator(plt.MaxNLocator(epochs))
plt.legend(["Train", "Test"], loc="upper left")
plt.grid()
plt.show()

"""##Predicted Values"""

# Get the predicted values
y_pred_scaled = model.predict(x_test)

# Unscale the predicted values
y_pred = scaler_pred.inverse_transform(y_pred_scaled)
y_test_unscaled = scaler_pred.inverse_transform(y_test.reshape(-1, 1))

"""## Evaluate Model Performance"""

#Regression_metrics
Regression_metrics(y_test, y_pred)

#classification_metrics
#classification_metrics(y_test, y_pred)

"""Plot"""

# Create a plot with regressor, true data and predictions
plt.figure(figsize=(20, 5))
plt.scatter(range(y_test_unscaled.shape[0]), y_test_unscaled, marker='+', label='true')
plt.scatter(range(y_pred.shape[0]), y_pred, marker='o', label='predicted')
plt.ylabel('Values')
plt.xlabel('Observation rows')
plt.title('True vs. predicted values')
plt.legend(loc='lower right')
plt.show()



"""## The Prediction Plot"""

# The date from which on the date is displayed
display_start_date = "2020-6-01"

# Add the difference between the valid and predicted prices
train = pd.DataFrame(data_filtered_ext[col][:train2_len + 1]).rename(columns={col: 'train'})
valid = pd.DataFrame(data_filtered_ext[col][train2_len:]).rename(columns={col: 'valid'})
valid.insert(1, "predict", y_pred, True)
valid.insert(1, "residuals", valid["predict"] - valid["valid"], True)
df_union = pd.concat([train, valid])

# Zoom in to a closer timeframe
df_union_zoom = df_union[df_union.index > display_start_date]

# Create the lineplot
plt.style.use("fivethirtyeight")
fig, ax1 = plt.subplots(figsize=(16, 8))
plt.title("predict vs valid")
plt.ylabel(1301, fontsize=18)
sns.set_palette(["#090364", "#1960EF", "#EF5919"])
sns.lineplot(data=df_union_zoom[['predict', 'train', 'valid']], linewidth=3.0, dashes=False, ax=ax1)





import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.preprocessing import MinMaxScaler

# Generate synthetic time series data
np.random.seed(0)
num_samples = 1000
time = np.arange(0, num_samples)
sin_wave = np.sin(0.1 * time) + np.random.normal(scale=0.1, size=num_samples)

# Normalize the data
scaler = MinMaxScaler()
normalized_data = scaler.fit_transform(sin_wave.reshape(-1, 1))

# Define a WindowGenerator class
class WindowGenerator:
    def __init__(self, input_width, label_width, shift, train_df, test_df):
        self.input_width = input_width
        self.label_width = label_width
        self.shift = shift
        self.train_df = train_df
        self.test_df = test_df

    def split_window(self, features):
        inputs = features[:, :-self.label_width]
        labels = features[:, -self.label_width:]
        labels = tf.stack([labels], axis=-1)
        return inputs, labels

    def make_dataset(self, data):
        data = np.array(data, dtype=np.float32)
        ds = tf.keras.preprocessing.timeseries_dataset_from_array(
            data=data,
            targets=None,
            sequence_length=self.input_width + self.label_width,
            sequence_stride=1,
            shuffle=True,
            batch_size=32,
        )
        ds = ds.map(self.split_window)
        return ds

# Split the data into training and testing sets
train_size = int(0.8 * num_samples)
train_data = normalized_data[:train_size]
test_data = normalized_data[train_size:]

input_width = 24
label_width = 1
shift = 1

w = WindowGenerator(input_width, label_width, shift, train_data, test_data)

# Define a simple LSTM model
model = Sequential([
    LSTM(32, return_sequences=True),
    LSTM(16, activation='relu'),
    Dense(label_width)
])

# Compile the model
model.compile(optimizer=Adam(), loss='mse')

# Train the model
early_stopping = EarlyStopping(patience=10, restore_best_weights=True)
history = model.fit(w.make_dataset(train_data), epochs=100, callbacks=[early_stopping])

# Make predictions
test_input, test_label = w.split_window(test_data)
predictions = model.predict(test_input)

# Plot results
plt.figure(figsize=(10, 6))
plt.plot(scaler.inverse_transform(test_label[:, :, 0]), label='True')
plt.plot(scaler.inverse_transform(predictions[:, :, 0]), label='Predicted')
plt.legend()
plt.show()



#https://github.com/flo7up/relataly-public-python-tutorials/blob/master/003%20Time%20Series%20Forecasting%20-%20Univariate%20Model%20using%20Recurrent%20Neural%20Networks.ipynb





"""# Research Articles

1.   https://github.com/flo7up/relataly-public-python-tutorials/blob/master/003%20Time%20Series%20Forecasting%20-%20Univariate%20Model%20using%20Recurrent%20Neural%20Networks.ipynb

2.   https://github.com/flo7up/relataly-public-python-tutorials/blob/master/005%20Time%20Series%20Forecasting%20-%20Multi-step%20Rolling%20Forecasting.ipynb


3.   https://deepnote.com/@fridrik-hafdisarson/M3-GROUP-ASSIGNMENT-0a91a105-d950-4ead-be9c-62fc16f5cfe8


"""
